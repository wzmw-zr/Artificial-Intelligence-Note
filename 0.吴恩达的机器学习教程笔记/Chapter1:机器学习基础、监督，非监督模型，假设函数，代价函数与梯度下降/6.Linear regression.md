# Linear Regression Model

## 一、Use Batch Gradient descent to solve Linear regression

In Linear Regression Model, we have hypothesis and cost function like this:
$$
h_\theta(x)=\theta_0+\theta_1x\\
J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}))-y^{(i)})^2
$$
and in Gradient descent algorithm, we will repeat until convergence:
$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1)
$$
we can use gradient descent to minimize the squared error cost function.

So, after calculate the partial derivative term, we can get formula below:
$$
\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{{i}})\\
\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{{i}}).x^{(i)}
$$
so, the gradient descent algorithm becomes below:

repeat until convergence:
$$
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})\\
\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}).x^{(i)}
$$
