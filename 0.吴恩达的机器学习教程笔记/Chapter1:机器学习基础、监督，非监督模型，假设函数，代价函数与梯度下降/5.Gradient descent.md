# Gradient descent

## 一、Gradient descent Algorithm

Gradient descent is a general algorithm used all over the place in machine learning.

We tend to use gradient descent to minimize arbitrary functions.

Provided that we have function $J(\theta_0,\theta_1)$,and we want to get $\min_{\theta_0,\theta_1}J(\theta_0,\theta_1)$.

+ So, we will start with some $\theta_0,\theta_1$

+ keep changing $\theta_0,\theta_1$ to reduce $J(\theta_0,\theta_1)$,until we hopefully end up at a local minimum.



**The definition of Gradient Algorithm:**

repeat until convergence:

> converge:收敛 disverge:发散

$$
\theta_j:=\theta_j-\alpha \frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)
$$

Here, **$\alpha$ is called the learning rate**, it controls how big a step we take downhill with gradient descent.

**$\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)$ is called derivative term.**

In Gradient descent, we want to **simultaneously update $\theta_0$ and** $\theta_1$. To achieve this, we will calculate like this:
$$
temp0:=\theta_0 - \alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)\\
temp1:=\theta_1 - \alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)\\
\theta_0:=temp0\\
\theta_1:=temp1
$$

> In Gradient descent, we must implement simultaneous update. 

Gradient descent can converge to a local minimum, even with learning rate $\alpha$ fixed.As we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease $\alpha$ over time.

> in gradient descent, as we approach the local minimum, the gradient descent will automatically take smaller steps, because the derivative term will automatically get smaller.



**The drawback of Gradient descent:** it can be susceptible to local optima.

We call **"Batch" Gradient descent** because **in gradient descent like this, each step of gradient decent uses all the training examples.**

There are also other versions of gradient descent which don't look at the entire training set, but look at small subsets of the training set at a time.

